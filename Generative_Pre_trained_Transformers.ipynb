{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AliSaeed090/NLProc-Sem1-M-Large-Language-Models-for-Natural-Language-Understanding/blob/main/Generative_Pre_trained_Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5i_Mj2kqwtsv"
      },
      "source": [
        "# Understanding Generative Pre-trained Transformers (GPT)\n",
        "\n",
        "This  implements a simplified GPT model from scratch to understand its architecture.\n",
        "We'll build each component step by step with detailed explanations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZv7HQxPwtsx"
      },
      "source": [
        "Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-sZp27Fwtsy",
        "outputId": "c8e28899-a804-4b26-9a29-fdfeff3e1e40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.9.0+cu128\n",
            "CUDA available: True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmg7sxSSwtsz"
      },
      "source": [
        "Multi-Head Self-Attention Mechanism\n",
        "\n",
        "The core of the Transformer is self-attention. It allows the model to weigh the importance\n",
        "of different words in a sequence when processing each word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7txzt3lLwtsz",
        "outputId": "47590c55-928c-4fb0-c116-d0055f3bcab3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Multi-Head Attention defined\n"
          ]
        }
      ],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-Head Self-Attention mechanism.\n",
        "\n",
        "    Parameters:\n",
        "    - d_model: Dimension of the model (embedding size)\n",
        "    - num_heads: Number of attention heads\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "\n",
        "        self.q_linear = nn.Linear(d_model, d_model)\n",
        "        self.k_linear = nn.Linear(d_model, d_model)\n",
        "        self.v_linear = nn.Linear(d_model, d_model)\n",
        "        self.out_linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_len, d_model = x.shape\n",
        "\n",
        "        Q = self.q_linear(x)\n",
        "        K = self.k_linear(x)\n",
        "        V = self.v_linear(x)\n",
        "\n",
        "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "        attn_output = torch.matmul(attn_weights, V)\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
        "        attn_output = attn_output.view(batch_size, seq_len, d_model)\n",
        "        output = self.out_linear(attn_output)\n",
        "\n",
        "        return output\n",
        "\n",
        "print(\"✓ Multi-Head Attention defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbBxWmPOwts0"
      },
      "source": [
        "Position-wise Feed-Forward Network\n",
        "\n",
        "---\n",
        "Position-wise Feed-Forward Network (FFN) in GPT is like giving each word its own tiny calculator.\n",
        "\n",
        "After self-attention mixes information between words, the FFN takes each word vector separately and passes it through two small layers:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pT3BIFP-wts0",
        "outputId": "a8dc91cb-e46d-49ec-8be7-ed19d71fda37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Feed-Forward Network defined\n"
          ]
        }
      ],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.linear1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear2(x)\n",
        "        return x\n",
        "\n",
        "print(\"✓ Feed-Forward Network defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUm2-6ecwts0"
      },
      "source": [
        "Positional Encoding\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "it handles variable sequence lengths during generation.\n",
        "Transformers (like GPT) read all words at the same time, so they don’t naturally know which word comes first, second, third, etc.\n",
        "\n",
        "Positional Encoding is like giving each word a timestamp.\n",
        "\n",
        "Every word gets a small vector that says:\n",
        "“I am at position 1, 2, 3…”\n",
        "\n",
        "These vectors are added to the word embeddings.\n",
        "\n",
        "This helps the model understand order, like knowing the difference between:\n",
        "“Dogs chase cats” and “Cats chase dogs”."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvOaamxIwts0",
        "outputId": "c32c66c5-b2bf-4c3a-ba3a-301d5a058ec3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Positional Encoding defined\n"
          ]
        }
      ],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
        "                            (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "\n",
        "        self.register_buffer('pe', pe)\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Only add positional encoding up to the sequence length\n",
        "        seq_len = x.size(1)\n",
        "        x = x + self.pe[:, :seq_len, :]\n",
        "        return x\n",
        "\n",
        "print(\"✓ Positional Encoding defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4V-CTCg-wts1"
      },
      "source": [
        "Transformer Decoder Block\n",
        "\n",
        "In GPT, a decoder block has just two main parts:\n",
        "\n",
        "Masked Self-Attention\n",
        "\n",
        "Each token looks only at past tokens.\n",
        "\n",
        "This is how GPT predicts the next word.\n",
        "\n",
        "Feed-Forward Network (FFN)\n",
        "\n",
        "A small neural network that processes each token individually.\n",
        "\n",
        "Around both parts, GPT uses:\n",
        "\n",
        "Residual connections (skip connections)\n",
        "\n",
        "Layer normalization\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jH1TQF4Iwts1",
        "outputId": "9ba568ee-39d7-47c5-e447-d906accbee3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Transformer Block defined\n"
          ]
        }
      ],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        attn_output = self.attention(x, mask)\n",
        "        x = x + self.dropout1(attn_output)\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = x + self.dropout2(ff_output)\n",
        "        x = self.norm2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "print(\"✓ Transformer Block defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbwBjmwFwts1"
      },
      "source": [
        "Complete GPT Model\n",
        "\n",
        "context window handling during generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyKMx5ohwts1",
        "outputId": "979ca7e5-ed82-4ed1-f4fb-9f8104e5b8c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Complete GPT model defined\n"
          ]
        }
      ],
      "source": [
        "class GPT(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=512, num_heads=8, num_layers=6,\n",
        "                 d_ff=2048, max_len=512, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.max_len = max_len\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
        "\n",
        "        self.transformer_blocks = nn.ModuleList([\n",
        "            TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "        mask = torch.triu(torch.ones(sz, sz), diagonal=1)\n",
        "        mask = mask == 0\n",
        "        return mask\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len = x.shape\n",
        "\n",
        "        x = self.token_embedding(x) * math.sqrt(self.d_model)\n",
        "        x = self.pos_encoding(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        mask = self.generate_square_subsequent_mask(seq_len).to(x.device)\n",
        "\n",
        "        for transformer_block in self.transformer_blocks:\n",
        "            x = transformer_block(x, mask)\n",
        "\n",
        "        logits = self.fc_out(x)\n",
        "        return logits\n",
        "\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0):\n",
        "        \"\"\"\n",
        "        Generate tokens autoregressively.\n",
        "        Handles context window by keeping only the most recent tokens.\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Crop context to max_len if needed\n",
        "            idx_cond = idx if idx.size(1) <= self.max_len else idx[:, -self.max_len:]\n",
        "\n",
        "            # Get predictions\n",
        "            logits = self.forward(idx_cond)\n",
        "\n",
        "            # Focus on last time step\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "\n",
        "            # Apply softmax\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "            # Sample\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            # Append to sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx\n",
        "\n",
        "print(\"✓ Complete GPT model defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rz7RCuibwts2"
      },
      "source": [
        "Create Simple Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VytDXmm2wts2",
        "outputId": "1493c912-1c53-4a6f-8345-a2c3ed2bb615"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Tokenizer defined\n"
          ]
        }
      ],
      "source": [
        "class SimpleTokenizer:\n",
        "    def __init__(self, text):\n",
        "        chars = sorted(list(set(text)))\n",
        "        self.vocab_size = len(chars)\n",
        "        self.char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
        "        self.idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "        print(f\"Vocabulary size: {self.vocab_size}\")\n",
        "        print(f\"Vocabulary: {''.join(chars)}\")\n",
        "\n",
        "    def encode(self, text):\n",
        "        return [self.char_to_idx[ch] for ch in text]\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        return ''.join([self.idx_to_char[idx] for idx in tokens])\n",
        "\n",
        "print(\"✓ Tokenizer defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhHlx4WDwts2"
      },
      "source": [
        "Prepare Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEOnymzpwts2",
        "outputId": "4da81363-d87f-4f6e-ce10-31ce13dfb207"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 35\n",
            "Vocabulary: \n",
            " ,.ADFINOTWabcdefghiklmnopqrstuvwy\n",
            "\n",
            "Encoded text length: 528 tokens\n"
          ]
        }
      ],
      "source": [
        "training_text = \"\"\"To be or not to be, that is the question.\n",
        "Whether it is nobler in the mind to suffer\n",
        "The slings and arrows of outrageous fortune,\n",
        "Or to take arms against a sea of troubles\n",
        "And by opposing end them. To die, to sleep,\n",
        "No more, and by a sleep to say we end\n",
        "The heartache and the thousand natural shocks\n",
        "That flesh is heir to. It is a consummation\n",
        "Devoutly to be wished. To die, to sleep,\n",
        "To sleep, perchance to dream. Ay, there is the rub,\n",
        "For in that sleep of death what dreams may come\n",
        "When we have shuffled off this mortal coil.\"\"\"\n",
        "\n",
        "tokenizer = SimpleTokenizer(training_text)\n",
        "encoded_text = tokenizer.encode(training_text)\n",
        "print(f\"\\nEncoded text length: {len(encoded_text)} tokens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dqxi-Oiqwts3"
      },
      "source": [
        "Create Training Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQeBfIwPwts3",
        "outputId": "38f81dd2-d8d1-4c68-f91c-df065b7a7d86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([16, 31])\n",
            "Target shape: torch.Size([16, 31])\n",
            "\n",
            "Example input:  To be or not to be, that is the\n",
            "Example target: o be or not to be, that is the \n"
          ]
        }
      ],
      "source": [
        "def create_dataset(encoded_text, seq_len, batch_size):\n",
        "    data = torch.tensor(encoded_text, dtype=torch.long)\n",
        "    num_sequences = len(data) // seq_len\n",
        "    data = data[:num_sequences * seq_len]\n",
        "    data = data.view(-1, seq_len)\n",
        "    inputs = data[:, :-1]\n",
        "    targets = data[:, 1:]\n",
        "    return inputs, targets\n",
        "\n",
        "seq_len = 32\n",
        "batch_size = 4\n",
        "\n",
        "inputs, targets = create_dataset(encoded_text, seq_len, batch_size)\n",
        "print(f\"Input shape: {inputs.shape}\")\n",
        "print(f\"Target shape: {targets.shape}\")\n",
        "print(f\"\\nExample input:  {tokenizer.decode(inputs[0].tolist())}\")\n",
        "print(f\"Example target: {tokenizer.decode(targets[0].tolist())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEWlMXrmwts3"
      },
      "source": [
        "Initialize Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIURjAvQwts3",
        "outputId": "92706ddb-327e-4ed2-8ddc-c943543dfa31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model parameters: 603,811\n",
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "model = GPT(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    d_model=128,\n",
        "    num_heads=4,\n",
        "    num_layers=3,\n",
        "    d_ff=512,\n",
        "    max_len=128,  # Larger context window for generation\n",
        "    dropout=0.1\n",
        ")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Model parameters: {total_params:,}\")\n",
        "print(f\"Device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1xIl-8awts3"
      },
      "source": [
        "Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5AcKTD0wts4",
        "outputId": "de0e92fa-562f-4696-d48a-8136b2f0590a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "\n",
            "Epoch 50/500, Loss: 0.7647\n",
            "Epoch 100/500, Loss: 0.1926\n",
            "Epoch 150/500, Loss: 0.0940\n",
            "Epoch 200/500, Loss: 0.0763\n",
            "Epoch 250/500, Loss: 0.0442\n",
            "Epoch 300/500, Loss: 0.0432\n",
            "Epoch 350/500, Loss: 0.0431\n",
            "Epoch 400/500, Loss: 0.0233\n",
            "Epoch 450/500, Loss: 0.0269\n",
            "Epoch 500/500, Loss: 0.0245\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "num_epochs = 500\n",
        "print_every = 50\n",
        "\n",
        "inputs = inputs.to(device)\n",
        "targets = targets.to(device)\n",
        "\n",
        "print(\"Starting training...\\n\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "\n",
        "    logits = model(inputs)\n",
        "    loss = criterion(logits.reshape(-1, tokenizer.vocab_size), targets.reshape(-1))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % print_every == 0:\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "print(\"\\nTraining complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78upTd_ewts4"
      },
      "source": [
        "Text Generation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHoCXaBQwts4",
        "outputId": "97a69015-dad7-4e68-88f6-1e5532d3fbff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Generation function ready\n"
          ]
        }
      ],
      "source": [
        "def generate_text(model, tokenizer, prompt, max_length=100, temperature=0.8):\n",
        "    model.eval()\n",
        "    tokens = tokenizer.encode(prompt)\n",
        "    tokens = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generated_tokens = model.generate(tokens, max_length, temperature)\n",
        "\n",
        "    generated_text = tokenizer.decode(generated_tokens[0].cpu().tolist())\n",
        "    return generated_text\n",
        "\n",
        "print(\"✓ Generation function ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5fmx_ylwts4"
      },
      "source": [
        "## Step 13: Test Text Generation!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_6Yc0znwts4",
        "outputId": "8061e22b-7714-4e04-8758-b2ed94a88622"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "TEXT GENERATION EXAMPLES\n",
            "============================================================\n",
            "\n",
            "Prompt: 'To be'\n",
            "Generated: 'To be or not to be, that is the the the the the the the'\n",
            "------------------------------------------------------------\n",
            "\n",
            "Prompt: 'To die'\n",
            "Generated: 'To die, to ro s ro ragobe, co ng to co ingato to to to t'\n",
            "------------------------------------------------------------\n",
            "\n",
            "Prompt: 'The'\n",
            "Generated: 'The ort be, t is the the the the thathe the the the t'\n",
            "------------------------------------------------------------\n",
            "\n",
            "Prompt: 'To be' (temperature=0.3, more deterministic)\n",
            "Generated: 'To be or not to be, that is the the the the the the the'\n",
            "------------------------------------------------------------\n",
            "\n",
            "Prompt: 'To be' (temperature=1.5, more random)\n",
            "Generated: 'To be or not to be, that is the the the the the the the'\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"TEXT GENERATION EXAMPLES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Example 1\n",
        "prompt1 = \"To be\"\n",
        "generated1 = generate_text(model, tokenizer, prompt1, max_length=50, temperature=0.8)\n",
        "print(f\"\\nPrompt: '{prompt1}'\")\n",
        "print(f\"Generated: '{generated1}'\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Example 2\n",
        "prompt2 = \"To die\"\n",
        "generated2 = generate_text(model, tokenizer, prompt2, max_length=50, temperature=0.8)\n",
        "print(f\"\\nPrompt: '{prompt2}'\")\n",
        "print(f\"Generated: '{generated2}'\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Example 3\n",
        "prompt3 = \"The\"\n",
        "generated3 = generate_text(model, tokenizer, prompt3, max_length=50, temperature=0.8)\n",
        "print(f\"\\nPrompt: '{prompt3}'\")\n",
        "print(f\"Generated: '{generated3}'\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Example 4: Lower temperature\n",
        "prompt4 = \"To be\"\n",
        "generated4 = generate_text(model, tokenizer, prompt4, max_length=50, temperature=0.3)\n",
        "print(f\"\\nPrompt: '{prompt4}' (temperature=0.3, more deterministic)\")\n",
        "print(f\"Generated: '{generated4}'\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Example 5: Higher temperature\n",
        "prompt5 = \"To be\"\n",
        "generated5 = generate_text(model, tokenizer, prompt5, max_length=50, temperature=1.5)\n",
        "print(f\"\\nPrompt: '{prompt5}' (temperature=1.5, more random)\")\n",
        "print(f\"Generated: '{generated5}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wZkA_Fiwts5"
      },
      "source": [
        "## Step 14: Try Your Own Prompts!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4Rjn2iswts5",
        "outputId": "d141a511-b720-46f8-e649-b465f37ab74c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your prompt: 'hi'\n",
            "Generated: 'hin to a sufond flertheind cheFond che he mindr ming mings mings ming ming ming mings mings matoby s s'\n"
          ]
        }
      ],
      "source": [
        "# Try your own prompts here!\n",
        "your_prompt = \"hi\"  # Change this to any text\n",
        "generated = generate_text(model, tokenizer, your_prompt, max_length=100, temperature=0.8)\n",
        "print(f\"Your prompt: '{your_prompt}'\")\n",
        "print(f\"Generated: '{generated}'\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0h2widZQBffe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MboJ_03zBfvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlxas4wjwts5"
      },
      "source": [
        "## Summary\n",
        "\n",
        "### What I Built:\n",
        "\n",
        " Complete GPT architecture from scratch  \n",
        "Character-level tokenizer  \n",
        "Training loop with next-token prediction  \n",
        "Text generation with temperature control  \n",
        "Trained on Shakespeare text  \n",
        "\n",
        "### How GPT Works:\n",
        "\n",
        "1. **Input**: Sequence of token IDs\n",
        "2. **Embedding**: Tokens → dense vectors\n",
        "3. **Positional Encoding**: Add position information\n",
        "4. **Transformer Blocks**: Self-attention + Feed-forward\n",
        "5. **Output**: Predict next token probabilities\n",
        "6. **Generation**: Sample tokens autoregressively\n",
        "\n",
        "### Key Innovation:\n",
        "\n",
        "Causal self-attention allows the model to learn context from previous tokens while preventing it from looking ahead.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}